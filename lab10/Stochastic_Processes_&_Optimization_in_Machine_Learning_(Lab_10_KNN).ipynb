{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Στοχαστικές Διεργασίες και Βελτιστοποίηση στη Μηχανική Μάθηση\n",
    "\n",
    "## 10ο Εργαστήριο - *Ο Αλγόριθμος KNN*\n",
    "\n",
    "- Ονομ/νυμο: Χρήστος Νίκου\n",
    "- AM: 03400146\n",
    "- Ιδιότητα: Μεταπτυχιακός φοιτητής Επιστήμης Δεδομένων και Μηχανικής Μάθησης (ΕΔΕΜΜ)\n",
    "- Ηλεκτρονική Διεύθυνση: christosnikou@mail.ntua.gr / chrisnick92@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Sc2hbFf_5x4"
   },
   "source": [
    "# Αλγόριθμος ΚΝΝ\n",
    "\n",
    "Στο παρακάτω παράδειγμα θα εξετάσουμε πως λειτουργεί ο αλγόριθμος Κ-Nearest Neighbors ([KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?msclkid=0c75f966cf9c11ecbab5cba311a90428)), χρησιμοποιώντας το [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set?msclkid=33057869cf9c11ec8e488a45734cbc68) σε προβλήματα κατηγοριοποίησης και προβλέψεων. Το αρχείο που θα χρησιμοποιήσετε είναι διαθέσιμο [**εδώ**](https://github.com/nkostopoulos/StochasticsLabPublic/blob/master/lab10/KNN/iris.csv).\n",
    "\n",
    "\n",
    "### Ερωτήσεις\n",
    "\n",
    "\n",
    "\n",
    "*   Να περιγράψετε συνοπτικά το τρόπο λειτουργίας του αλγορίθμου ΚΝΝ\n",
    "*   Ποια είναι τα πλεονεκτήματα και τα μειονεκτήματα του συγκεκριμένου αλγορίθμου; \n",
    "*   Ο συγκεκριμένος αλγόριθμος είναι αποδοτικός στην περίπτωση που έχουμε μεγάλο πλήθος μεταβλητών; Τι συμβαίνει στις περιπτώσεις όπου επιλέξουμε μεγάλο ή μικρό k; \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτηση 1\n",
    "\n",
    ">*Να περιγράψετε συνοπτικά το τρόπο λειτουργίας του αλγορίθμου ΚΝΝ.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O KNN είναι ένας μη παραμετρικός ταξινομητής βασισμένος σε παραδείγματα (instance-based). Η αρχή λειτουργίας του είναι πολύ απλή. Για ένα νέο δείγμα προς ταξινόμηση, πρώτα υπολογίζουμε τους k πλησιέστερους γείτονές του (στον ν-διάστατο χώρο των χαρακτηριστικών εισόδου) με βάση κάποια συνάρτηση απόστασης, συνήθως ευκλείδεια\n",
    "$$d(x, x') = \\sqrt{\\left(x_1 - x'_1 \\right)^2 + \\left(x_2 - x'_2 \\right)^2 + \\dotsc + \\left(x_n - x'_n \\right)^2}$$\n",
    "Η κλάση του νέου δείγματος θα είναι η κλάση της πλειοψηφίας των k γειτόνων (διαλέγουμε k περιττό γενικά), είτε απλά υπολογισμένη (άθροισμα) είτε (αντίστροφα) ζυγισμένη με βάση την απόσταση του κάθε γείτονα. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτηση 2\n",
    "\n",
    ">*Ποια είναι τα πλεονεκτήματα και τα μειονεκτήματα του συγκεκριμένου αλγορίθμου;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ο KNN δεν έχει πρακτικά φάση εκπαίδευσης. Ωστόσο, για να ταξινομήσουμε ένα νέο δείγμα στην φάση test,  πρέπει να συγκρίνουμε την απόστασή του με κάθε δείγμα του train set. Αυτό σημαίνει ότι για την ταξινόμηση είναι απαραίτητα όλα τα δείγματα εκπαίδευσης (εξού και η ονομασία \"instance-based\", ενώ στον Naive Bayes χρειαζόμαστε μόνο τις παραμέτρους $μ$ και $σ^2$). Αυτό σημαίνει ότι ο KNN είναι πιο απαιτητικός και σε χώρο (αποθήκευση όλων των δειγμάτων) και σε χρόνο (υπολογισμός όλων των αποστάσεων για κάθε νέο δείγμα).\n",
    "\n",
    "Ένα άλλο μειονέκτημα του KNN είναι μια υπερπαράμετρος $Κ$ του ταξινομητή. Το πρόβλημα είναι με το $K$ είναι ότι η επιλογή της γίνονται από τον σχεδιαστή του συστήματος και έτσι δεν μπορούμε να ξέρουμε εκ των προτέρων τις βέλτιστες τιμές του $K$ χωρίς να την αξιολογήσουμε εμπειρικά.\n",
    "\n",
    "Τώρα, τα πλεονεκτήματα του KNN είναι ότι είναι απλός και κατανοητός ως προς την υλοποίησή του. Επίσης, το γεγονός ότι ο ΚΝΝ είναι ένας μη παραμετρικός αλγόριθμος σημαίνει πως δεν χρειάζεται να ελεγχούν αν τα δεδομένα μας ικανοποιούν κάποιες υποθέσεις (όπως για παράδειγμα στη γραμμική παλινδρόμηση). Όσο περισσότερα δείγματα έχουμε τότε ο αλγόριθμος βελτιώνεται καθώς μπορεί να βγάλει καλύτερα συμπεράσματα για τις διαφορετικές κλάσεις. Τέλος, υπάρχει ελευθερία ως προς την επιλογή της απόστασης που θέλουμε να χρησιμοποιήσουμε ανάλογα με το πρόβλημα που θέλουμε να λύσουμε (π.χ. Ευκλείδεια, Manhattan, Minkowski κτλ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτηση 3\n",
    "\n",
    ">Ο συγκεκριμένος αλγόριθμος είναι αποδοτικός στην περίπτωση που έχουμε μεγάλο πλήθος μεταβλητών; Τι συμβαίνει στις περιπτώσεις όπου επιλέξουμε μεγάλο ή μικρό k;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Στην περίπτωση που έχουμε μεγάλο πλήθος μεταβλητών τότε αλγόριθμος δεν είναι ιδιαίτερα απόδοτικός. Αυτό συμβαίνει γιατί ο υπολογίσμός μεταξύ των αποστάσεων δύο σημείων σε έναν πολυδιάστατο χώρο πολλών μεταβλητών είναι ιδιαίτερα χρονοβόρος.<br>\n",
    "\n",
    "- Στη περίπτωση που επιλέξουμε μεγάλη τιμή για την υπερπαράμετρο $K$ τότε αυξάνουμε των αριθμών των γειτόνων που πρέπει να εντοπιστούν από μια κλάση έτσι ώστε να κατηγοριοποιήσουμε το νέο δείγμα. Αυτό σημαίνει, ότι για να βρεθούν οι $K$ γείτονες ενδεχομένως να λάβουμε υπ'όψιν σημεία που βρίσκονται αρκετά μακριά από το δείγμα προς ταξινόμηηση με αποτέλεσμα το δείγμα να κατηγοριοποιηθεί σε μια κλάση στην οποία να βρίσκονται ανομοιογενή δείγματα σε σχέση με το σημείο προς ταξινόμηση. Επίσης, όσο μεγαλώνουμε τον αριμό του $K$ αυξάνουμε και τον χρόνο εκτέλεσης του αλγορίθμου καθώς ο εντοπισμός των $K$ γειτόνων θα θέλει περισσότερη ώρα.\n",
    "\n",
    "- Για πολύ μικρές τιμές του $K$ μειώνουμε αρκετά τον χρόνο εκτέλεσης του αλγορίθμου. Παρ' όλα αυτά, αν έχουμε αρκετά μικρό $K$ παίρνουμε ένα ταξινομητή με υψηλή διακύμανση και χαμηλή απόκλιση. Ο ταξινομητής τείνει να αγνοεί τη συνολική κατανομή και αποφασίζει μόνο από το κοντινότερο δείγμα.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YdDd567UDrAH"
   },
   "outputs": [],
   "source": [
    "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z0S9ZQ57Dv8-"
   },
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2APEXH9DD3S_"
   },
   "source": [
    "\n",
    "\n",
    "*   Αναφέρετε άλλες μεθόδους υπολογισμού της απόστασης\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως βλέπουμε και στο παρακάτω κελί, η απόσταση που χρησιμοποιείται στη συγκεκριμένη υλοποίηση του KNN είναι η Ευκλείδεια απόσταση που υπολογίζεται μέσω της συνάρτησης `euclidean_distance`. Άλλες αποστάσεις που μπορούν να χρησιμοποιηθούν είναι οι:<br>\n",
    "\n",
    "<b>1.</b> Η $\\ell_1$ ή απόσταση Manhattan η οποία δίνεται μέσω της<br>\n",
    "\n",
    "$$\\ell_1(\\mathbf{x},\\mathbf{y}) = \\sum_{i=1}^{n}|x_i-y_i|,$$\n",
    "\n",
    "όπου $\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^n$, με $\\mathbf{x}=(x_1,\\dots,x_n),\\, \\mathbf{y}=(y_1,\\dots,y_n)$.\n",
    "\n",
    "<b>2. </b> Γενίκοτερα, μπορεί να χρησιμοποιήθεί η απόσταση Minkowski που δίνεται μέσω της\n",
    "$$\\ell_p(\\mathbf{x},\\mathbf{y}) = \\biggl(\\sum_{i=1}^{n}|x_i-y_i|^p\\biggr)^{1/p},$$\n",
    "\n",
    "$p\\geq 1$ και $\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^n$.\n",
    "\n",
    "<b>3. </b> Άλλη απόσταση είναι είναι $\\ell_{\\infty}$ που δίνεται μέσω της \n",
    "$$\\ell_{\\infty}(\\mathbf{x},\\mathbf{y}) = \\max_{1\\leq i \\leq n}|x_i-y_i|,$$\n",
    "για $\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^n.$\n",
    "\n",
    "<b>4.</b> Η απόσταση ομοιόμητας συνημιτόνου που δίνεται μέσω της \n",
    "$$\\text{cosine similarity}= \\frac{\\langle \\mathbf{x},\\mathbf{y}\\rangle}{||\\mathbf{x}||_2\\cdot ||\\mathbf{y}||_2},$$\n",
    "για $\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R4OjL9p18HL-"
   },
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIVzwwpSEeE-"
   },
   "source": [
    "\n",
    "\n",
    "*   Δοκιμάστε με τα παρακάτω ζεύγη τιμών και σημειώστε τα αποτελέσματα\n",
    "\n",
    "\n",
    "*   [4.9, 3.1, 1.5, 0.1]\n",
    "*   [6.9, 3.1, 4.9, 1.5]\n",
    "*   [5.0, 2.0, 3.5, 1.0]\n",
    "*   [5.6, 2.7, 4.2, 1.3]\n",
    "*   [6.3, 3.3, 6.0, 2.5]\n",
    "*   [5.7, 2.9, 4.2, 1.3]\n",
    "*   [5.9, 3.0, 5.1, 1.8]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iris-setosa] => 0\n",
      "[Iris-virginica] => 1\n",
      "[Iris-versicolor] => 2\n",
      "-----> Classification with Euclidean Distance <-----\n",
      "Data=[4.9, 3.1, 1.5, 0.1], Predicted: 0\n",
      "Data=[6.9, 3.1, 4.9, 1.5], Predicted: 2\n",
      "Data=[5.0, 2, 3.5, 1], Predicted: 2\n",
      "Data=[5.6, 2.7, 4.2, 1.3], Predicted: 2\n",
      "Data=[6.3, 3.3, 6, 2.5], Predicted: 1\n",
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 2\n",
      "Data=[5.9, 3.0, 5.1, 1.8], Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "rows = [[4.9,3.1,1.5,0.1], [6.9,3.1,4.9,1.5], [5.0,2,3.5,1],\n",
    "       [5.6,2.7, 4.2, 1.3], [6.3,3.3,6,2.5], [5.7,2.9,4.2,1.3], [5.9,3.0,5.1,1.8]]\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "print(f\"{5*'-'}> Classification with Euclidean Distance <{5*'-'}\")\n",
    "for row in rows:\n",
    "    label = predict_classification(dataset, row, num_neighbors)\n",
    "    print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στον παρακάτω πίνακα συνοψίζουμε την κατηγοριοποίηση των παραπάνω 7 σημείων.\n",
    "\n",
    "|**sepal length in cm** | **sepal width in cm** |**petal length in cm**|**petal width in cm**|**Category**|\n",
    "|:---------------|:-----------------|:-----------------|:-----------------|:-----------------|\n",
    "| **4.9** |       **3.1**     | **1.5**| **0.1**| **Iris-setosa**|\n",
    "| **6.9** |       **3.1**     | **4.9**| **1.5**| **Iris-versicolor**|\n",
    "| **5.0** |       **2**     | **3.5**| **1**| **Iris-versicolor**|\n",
    "| **5.6** |       **2.7**     | **4.2**| **1.3**| **Iris-versicolor**|\n",
    "| **6.3** |       **3.3**     | **6**| **2.5**| **Iris-virginica**|\n",
    "| **5.7** |       **2.9**     | **4.2**| **1.3**| **Iris-versicolor**|\n",
    "| **5.9** |       **3.0**     | **5.1**| **1.8**| **Iris-virginica**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vVci7H7jExef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iris-setosa] => 0\n",
      "[Iris-virginica] => 1\n",
      "[Iris-versicolor] => 2\n",
      "Data=[4.5, 2.3, 1.3, 0.3], Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "# Make a prediction with KNN on Iris Dataset\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "# define a new record\n",
    "row = [4.5, 2.3, 1.3, 0.3]\n",
    "# predict the label\n",
    "label = predict_classification(dataset, row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9__nuQqGGyN4"
   },
   "source": [
    "\n",
    "\n",
    "*   Δοκιμάστε να υπολογίσετε την απόσταση με τη μετρική [Manhattan](https://iq.opengenus.org/manhattan-distance/#:~:text=Manhattan%20distance%20is%20a%20distance%20metric%20between%20two,the%20measures%20in%20all%20dimensions%20of%20two%20points.?msclkid=50bbf70ecfa011ec91862d6b9263d761) για τα ζέυγη που σας έχουν δοθεί. Παρατηρείτε κάποια διαφοροποίηση ως προς τα αποτελέσματα;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Manhattan distance between two vectors\n",
    "def manhattan_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += abs(row1[i] - row2[i])\n",
    "\treturn distance\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors2(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = manhattan_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification2(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors2(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iris-setosa] => 0\n",
      "[Iris-virginica] => 1\n",
      "[Iris-versicolor] => 2\n",
      "Data=[4.5, 2.3, 1.3, 0.3], Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "# Make a prediction with KNN on Iris Dataset\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "# define a new record\n",
    "row = [4.5, 2.3, 1.3, 0.3]\n",
    "# predict the label\n",
    "label = predict_classification2(dataset, row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Classification with Manhattan Distance <-----\n",
      "[Iris-setosa] => 0\n",
      "[Iris-virginica] => 1\n",
      "[Iris-versicolor] => 2\n",
      "Data=[4.9, 3.1, 1.5, 0.1], Predicted: 0\n",
      "Data=[6.9, 3.1, 4.9, 1.5], Predicted: 2\n",
      "Data=[5.0, 2, 3.5, 1], Predicted: 2\n",
      "Data=[5.6, 2.7, 4.2, 1.3], Predicted: 2\n",
      "Data=[6.3, 3.3, 6, 2.5], Predicted: 1\n",
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 2\n",
      "Data=[5.9, 3.0, 5.1, 1.8], Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{5*'-'}> Classification with Manhattan Distance <{5*'-'}\")\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "for row in rows:\n",
    "    label = predict_classification2(dataset, row, num_neighbors)\n",
    "    print('Data=%s, Predicted: %s' % (row, label))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως βλέπουμε και απ τα παραπάνω αποτελέσματα, χρησιμοποιώντας την απόσταση Manhattan αντί της Ευκλείδειας δε βλέπουμε κάποια αλλαγή ως προς το αποτέλεσμα της ταξινόμησης καθώς έχουμε ακριβώς τα ίδια αποτελέσματα."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_10_KNN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
