{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Στοχαστικές Διεργασίες και Βελτιστοποίηση στη Μηχανική Μάθηση\n",
    "\n",
    "## 6ο Εργαστήριο - *Restricted Boltzmann Machine και Deep Belief Network*\n",
    "\n",
    "- Ονομ/νυμο: Χρήστος Νίκου\n",
    "- AM: 03400146\n",
    "- Ιδιότητα: Μεταπτυχιακός φοιτητής Επιστήμης Δεδομένων και Μηχανικής Μάθησης (ΕΔΕΜΜ)\n",
    "- Ηλεκτρονική Διεύθυνση: christosnikou@mail.ntua.gr / chrisnick92@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5q7C447X8Up"
   },
   "source": [
    "<h1><b>Restricted Boltzmann Machine και Deep Belief Network</b></h1>\n",
    "<p align=\"justify\">Στην συγκεκριμένη άσκηση θα μελετήσετε τον τρόπο λειτουργίας μιας <i>RBM (<a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">Restricted Boltzmann Machine</a>)</i> καθώς και των <i>DBN (<a href=\"https://en.wikipedia.org/wiki/Deep_belief_network\">Deep Belief Network</a>)</i>, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.Το συγκεκριμένο πρόγραμμα αξιοποιεί το <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">dataset του <i>MNIST</i></a>, όπου είναι μια μεγάλη βάση δεδομένων με χειρόγραφα ψηφία που χρησιμοποιείται συνήθως για την εκπαίδευση διαφόρων συστημάτων επεξεργασίας εικόνας. Για την άσκηση, θα πρέπει να χρησιμοποιήσετε το αρχείο <i>mnist_original.mat</i>, το οποίο είναι διαθέσιμο από <a href=\"https://www.kaggle.com/datasets/avnishnish/mnist-original?resource=download\">εδώ</a>.</p>\n",
    "<p align=\"justify\">Μία αρκετά σημαντική εφαρμογή της <i>RBM</i> είναι η εξαγωγή χαρακτηριστικών (feature representation) από ένα dataset με σκοπό την αναπαράσταση της εισόδου (ορατοί νευρώνες) με ένα διάνυσμα μικρότερης διάστασης (κρυφοί νευρώνες). Στη συγκεκριμένη άσκηση θα συγκρίνετε την ακρίβεια ενός ταξινομητή ψηφίων με τη χρήση του αλγορίθμου <i>Logistic Regression</i>, όταν εκείνος δέχεται ως είσοδο το dataset (i) χωρίς να έχει υποστεί επεξεργασία από το <i>RBM</i>, (ii) αφου υποστεί επεξεργασία από το <i>RBM</i>, (iii) με τη χρήση <i>DBN</i>, δηλαδή δύο stacked <i>RBM</i>.</p>\n",
    "<p align=\"justify\"> Με βάση τον κώδικα που σας έχει δοθεί, καλείστε να απαντήσετε στα παρακάτω ερωτήματα:</p>\n",
    "<ul>\n",
    "<li>Να περιγράψετε σύντομα τον τρόπο λειτουργίας μιας <i>RBM</i>. Τι διαφορές έχει σε σχέση με μία <i> Μηχανή Boltzmann</i>;</li>\n",
    "<li>Ποια είναι η λογική των <i>DBN</i> και σε τι προβλήματα τα αξιοποιούμε;</li>\n",
    "<li>Να αναφέρετε τις βασικότερες εφαρμογές των <i>RBM</i> και <i>DBN</i>.</li>\n",
    "<li>Εκτός από <i>RBM</i>, τι άλλα μοντέλα μπορούν να χρησιμοποιηθούν για να δημιουργήσουν <i>DBN</i>.</li>\n",
    "<li>Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo <i>Logistic Regression</i> χωρίς τη χρήση <i>RBM</i> σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η <i>RBM</i> καθώς και με αυτή όπου χρησιμοποιούνται <i>RBM</i> και <i>DBN</i> για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><b>1.</b> *Να περιγράψετε σύντομα τον τρόπο λειτουργίας μιας RBM. Τι διαφορές έχει σε σχέση με μία Μηχανή Boltzmann;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ένα <b>Restricted Boltzmann Machine (RBM)</b> είναι ένα generative στοχαστικό νευρωνικό δίκτυο το οποίο μαθαίνει την κατανομή των δεδομένων που του δίνονται. Μερικές απ'τις εφαρμογές των *RBM* είναι οι εξής:\n",
    "\n",
    "- Μείωση διαστατικότητας (*dimensionality reduction*)\n",
    "- Εκμάθηση χαρακτηριστικών (*feature learning*)\n",
    "- Ταξινόμηση (*classification*) και\n",
    "- Παλινδρόμηση (*regression*)\n",
    "\n",
    "Η διαφορά σε σχέση με μια *Μηχανή Boltzmann* είναι ότι δεν υπάρχουν συνδέσεις μεταξύ των δύο «ομάδων» νευρώνων. Δηλαδή, δεν υπάρχουν συνδέσεις μεταξύ των ορατών (*visible*) νευρώνων όπως δεν υπάρχουν συνδέσεις και μεταξύ δύο νευρώνων που ανήκουν στους κρυφούς νευρώνες (*hidden*). Οι συνδέσεις που υπάρχουν είναι μόνο μεταξύ νευρώνων που ανήκουν σε διαφορετικές κατηγορίες. Η διαφορά αυτή των *RBM* σε σχέση με τις *Μηχανές Boltzmann* επιτρέπει στα *RBM* να αξιοποιούν μεθόδους κατάβασης κλίσεως (*gradient based methods*).\n",
    "\n",
    "Πιο συγκεκριμένα, η εκπαίδευση μιας *RBM* αποτελείται από δύα στάδια. Το *forward pass* από το *layer* των φανερών νευρώνων προς το *layer* των κρυφών νευρώνων και το *backward pass* ακολουθώντας την αντίστροφη πορεία. Το γεγονός ότι οι *RBM* ανήκουν στην κατηγορία των μη επιβλεπόμενων μοντέλων σημαίνει δεν υπάρχει κάποια τιμή αναφοράς για να υπολογιστεί κάποιο σφάλμα μεταξύ της εξόδου του μοντέλου και της τιμής, αλλά αντίθετα, η *RBM* προσπαθεί προσεγγίσει την κατανομή των δεδομένων εισόδου $p(x)$ μέσω μιας ανακατασκευασμένης κατανομής $q(x)$. Η προσέγγιση αυτή γίνεται ελαχιστοποιώντας την απόκλιση *Kullback - Liebler* $D_{\\text{KL}}(P\\, ||\\, Q)$ της κατανομής $q$ από την $p$ η οποία δίνεται μέσω της ποσότητας\n",
    "\n",
    "$$D_{\\text{KL}}(P\\, ||\\, Q) = \\int p(x)\\log\\biggl(\\frac{p(x)}{q(x)}\\biggr)\\,dx.$$\n",
    "\n",
    "Ειδικότερα, η διαδικασία προσέγγισης της $p(x)$ ή αλλιώς $p(\\mathbf{v})$, με $\\mathbf{v}$ τα διανύσματα εισόδου, απαιτεί αρχικά των υπολογισμό των διανυσμάτων $\\mathbf{h}$ στο hidden layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(1)}=S\\left(\\mathbf{v}^{(0) T} W+\\mathbf{a}\\right)\n",
    "$$\n",
    "\n",
    "όπου $\\mathbf{v}^{(0)}$ είναι το διάνυσμα εισόδου, $W$ τα βάρη, $\\mathbf{a}$ τα biases του hidden layer και S η συνάρτηση softmax. Από τον συγκεκριμένο υπολογισμό προσεγγίζεται η κατανομή πιθανότητας:\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{h}^{(1)} \\mid \\mathbf{v}^{(0)} ; W\\right)\n",
    "$$\n",
    "\n",
    "Στη συνέχεια, υλοποιείται η φάση της ανακατασκευής του αρχικού διανύσματος:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^{(1)}=S\\left(\\mathbf{h}^{(1) T} W+\\mathbf{b}\\right)\n",
    "$$\n",
    "\n",
    "όπου $\\mathbf{b}$ τα biases του visible layer. Με αυτόν τον τρόπο προσεγγίζεται η κατανομή πιθανότητας:\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{v}^{(1)} \\mid \\mathbf{h}^{(1)} ; W\\right).\n",
    "$$\n",
    "\n",
    "Τώρα, για την προσέγγιση της $p(v)$ υπολογίζεται η «ενέργεια» $E(\\mathbf{v},\\mathbf{h})$ μεταξύ του ορατού layer και του κρυφού layer η οποία δίνεται μέσω της σχέσης\n",
    "\n",
    "$$\n",
    "E(\\mathbf{v}, \\mathbf{h})=-\\sum_{i \\in v i s i b l e} a_{i} v_{i}-\\sum_{j \\in \\text { hidden }} b_{j} h_{j}-\\sum_{i, j} v_{i} h_{j} w_{i j}\n",
    "$$\n",
    "\n",
    "όπου $v_{i}$ και $h_{j}$ είναι οι δυαδικές καταστάσεις του διανύσματος εισόδου από τον κόμβο i και του διανύσματος του hidden layer από τον κόμβο j αντίστοιχα, $a_{i}$ και $b_{j}$ είναι τα αντίστοιχα biases και $w_{ij}$ το βάρος μεταξύ τους. Τώρα, η προσέγγιση της $p(\\mathbf{v})$ γίνεται μέσω της\n",
    "\n",
    "$$\n",
    "p(\\mathbf{v})=\\frac{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}}{\\sum_{\\mathbf{v}, \\mathbf{h}} e^{-E(\\mathbf{v}, \\mathbf{h})}}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><b>2.</b> *Ποια είναι η λογική των DBN και σε τι προβλήματα τα αξιοποιούμε;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα Deep Belief Networks *(DBNs)* είναι generative μοντέλα που αποτελούνται από συνδυασμό πολλαπλών Restricted Boltzmann Machines *(RBMs)*, όπου για κάθε *RBM* layer που χρησιμοποιείται, το κρυφό layer του προηγούμενου *RBM* είναι το φανερό layer του επόμενου. Μετά το τελευταίο layer μπορεί να τοποθετηθεί ένα layer το οποίο να αφορά κάποια συνάρτηση ταξινόμησης, όπως π.χ. *logistic* ή *softmax*. Έτσι, τα *DBN* πέρα από προσέγγιση κατανομών των δεδομένων μπορούν να αξιοποιηθούν και για λόγους ταξινομήσης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><b>3 + 4.</b> *Να αναφέρετε τις βασικότερες εφαρμογές των RBM και DBN. Εκτός από RBM, τι άλλα μοντέλα μπορούν να χρησιμοποιηθούν για να δημιουργήσουν DBN.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μια από τις πιο βασικές εφαρμογές των *RBM* είναι η εκμάθηση της κατανομής των δεδομένων εισόδου όπως περιγράφηκε προηγουμένως. Η λογική των *RBM* βρίσκει εφαρμογές στα συστήματα που δημιουργούν προτάσεις για τους χρήστες διάφορων εφαρμογών (*recommendation systems*). Άλλες εφαρμογές έχουν να κάνουν με την εξαγωγή χρήσιμων χαρακτηριστικών και τη μείωση διαστατικότητας. Αντίστοιχα, τα *DBN* μπορούν να χρησιμοποιηθούν σε προβλήματα ταξινόμησης όπως και την αναγνώριση αντικειμένων (*object detection*). \n",
    "\n",
    "Άλλα μοντέλα που μπορούν να χρησιμοποιηθούν εκτός από τα *RBM* για να δημιουργήσουν *DBN* είναι οι *Autoencoders*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><b>5.</b> *Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo Logistic Regression χωρίς τη χρήση RBM σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η RBM καθώς και με αυτή όπου χρησιμοποιούνται RBM και DBN για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z6OQ6-N8ajZJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\dem0nakos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96       995\n",
      "         1.0       0.96      0.98      0.97      1121\n",
      "         2.0       0.90      0.90      0.90      1015\n",
      "         3.0       0.90      0.88      0.89      1033\n",
      "         4.0       0.93      0.92      0.92       976\n",
      "         5.0       0.90      0.88      0.89       884\n",
      "         6.0       0.94      0.94      0.94       999\n",
      "         7.0       0.92      0.93      0.92      1034\n",
      "         8.0       0.88      0.86      0.87       923\n",
      "         9.0       0.89      0.90      0.89      1020\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.91      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -140.98, time = 10.06s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -115.93, time = 13.97s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -108.03, time = 14.11s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -98.85, time = 14.56s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -93.13, time = 14.48s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -90.18, time = 16.06s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -85.79, time = 15.74s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -82.60, time = 14.86s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -80.93, time = 15.50s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -80.06, time = 15.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\dem0nakos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       995\n",
      "         1.0       0.98      0.99      0.99      1121\n",
      "         2.0       0.96      0.97      0.96      1015\n",
      "         3.0       0.98      0.95      0.97      1033\n",
      "         4.0       0.97      0.97      0.97       976\n",
      "         5.0       0.97      0.97      0.97       884\n",
      "         6.0       0.98      0.98      0.98       999\n",
      "         7.0       0.98      0.98      0.98      1034\n",
      "         8.0       0.96      0.95      0.96       923\n",
      "         9.0       0.95      0.96      0.96      1020\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -140.21, time = 11.17s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -117.96, time = 15.70s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -105.89, time = 15.29s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -100.23, time = 15.90s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -90.72, time = 16.17s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -88.89, time = 15.10s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -86.68, time = 16.86s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -83.21, time = 15.74s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -80.85, time = 15.19s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -77.37, time = 16.22s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -320.32, time = 11.45s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -285.85, time = 16.64s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -273.46, time = 15.67s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -261.95, time = 16.03s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -260.48, time = 17.54s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -260.56, time = 15.58s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -256.76, time = 17.80s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -255.40, time = 18.78s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -255.89, time = 16.95s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -253.43, time = 15.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\dem0nakos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       995\n",
      "         1.0       0.99      0.99      0.99      1121\n",
      "         2.0       0.97      0.98      0.98      1015\n",
      "         3.0       0.97      0.96      0.97      1033\n",
      "         4.0       0.98      0.97      0.97       976\n",
      "         5.0       0.97      0.97      0.97       884\n",
      "         6.0       0.99      0.98      0.98       999\n",
      "         7.0       0.98      0.98      0.98      1034\n",
      "         8.0       0.96      0.96      0.96       923\n",
      "         9.0       0.95      0.96      0.95      1020\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# source: https://devdreamz.com/question/905929-stacking-rbms-to-create-deep-belief-network-in-sklearn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def norm(arr):\n",
    "    arr = arr.astype(float)\n",
    "    arr -= arr.min()\n",
    "    arr /= arr.max()\n",
    "    return arr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # load MNIST data set\n",
    "    mnist = loadmat(\"mnist-original.mat\")\n",
    "    X, Y = mnist[\"data\"].T, mnist[\"label\"][0]\n",
    "\n",
    "    # normalize inputs to 0-1 range\n",
    "    X = norm(X)\n",
    "\n",
    "    # split into train, validation, and test data sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,       Y,       test_size=10000, random_state=0)\n",
    "    X_train, X_val,  Y_train, Y_val  = train_test_split(X_train, Y_train, test_size=10000, random_state=0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # set hyperparameters\n",
    "\n",
    "    learning_rate = 0.02 \n",
    "    total_units   =  800 \n",
    "    total_epochs  =  10\n",
    "    batch_size    =  128\n",
    "\n",
    "    C = 100. # optimum for benchmark model according to sklearn docs: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # construct models\n",
    "\n",
    "    # RBM\n",
    "    rbm = BernoulliRBM(n_components=total_units, learning_rate=learning_rate, batch_size=batch_size, n_iter=total_epochs, verbose=1)\n",
    "\n",
    "    # \"output layer\"\n",
    "    logistic = LogisticRegression(C=C, solver='lbfgs', multi_class='multinomial', max_iter=200, verbose=1)\n",
    "\n",
    "    models = []\n",
    "    models.append(Pipeline(steps=[('logistic', clone(logistic))]))                                              # base model / benchmark\n",
    "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('logistic', clone(logistic))]))                        # single RBM\n",
    "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('rbm2', clone(rbm)), ('logistic', clone(logistic))]))  # RBM stack / DBN\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # train and evaluate models\n",
    "\n",
    "    for model in models:\n",
    "        # train\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # evaluate using validation set\n",
    "        print(\"Model performance:\\n%s\\n\" % (\n",
    "            classification_report(Y_val, model.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο παραπάνω πρόγραμμα χρησιμοποιούνται τρία διαφορετικά μοντέλα για την ταξινόμηση του συνόλου δεδομένων *MNIST*, όπου είναι μια μεγάλη βάση δεδομένων με χειρόγραφα ψηφία που χρησιμοποιείται συνήθως για την εκπαίδευση διαφόρων συστημάτων επεξεργασίας εικόνας. Τα τρία μοντέλα είναι 1) Η λογιστική παλινδρόμηση χωρίς τη χρήση *RBN*, 2) η λογιστική παλινδρόμηση με τη χρήση *RBM* και η 3) η λογιστική παλινδρόμηση με *DBN* από 2 *RBM*. \n",
    "\n",
    "Όπως βλέπουμε και στις τρεις περιπτώσεις η ακριβεία βρίσκεται σε πλήρη αντιστοιχία ως προς τις μετρικές *accuracy, macro average* και *weighted average*. Στον παρακάτω πίνακα συνοψίζονται τα αποτελέσματα.\n",
    "\n",
    "| Μοντέλο | Ακρίβεια |\n",
    "| :---: | :---: |\n",
    "| Λογιστική παλινδρόμηση χωρίς RBM | **92%** |\n",
    "| Λογιστική παλινδρόμηση με RBM | **97%** |\n",
    "| Λογιστική παλινδρόμηση με DBN από 2 RBM | **97%** |\n",
    "\n",
    "Όπως βλέπουμε σε όλες τις περιπτώσεις η ακρίβεια ξεπερνάει το $90\\%$. Στη περίπτωση της λογιστική παλινδρόμησης έχουμε ακρίβεια $92%$ ενώ όπως βλέπουμε στις δύο περιπτώσεις που αξιοποιούνται τα *RBM* και τα *DBN* η ακρίβεια ανεβαίνει στο $97\\%$."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5_RBM_DBN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
