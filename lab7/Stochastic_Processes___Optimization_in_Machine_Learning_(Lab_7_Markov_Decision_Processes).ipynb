{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWJn8KBgugDh"
   },
   "source": [
    "# Στοχαστικές Διεργασίες και Βελτιστοποίηση στη Μηχανική Μάθηση\n",
    "\n",
    "## 7o Εργαστήριο - *Markov Descision Processes*\n",
    "\n",
    "- Ονομ/νυμο: Χρήστος Νίκου\n",
    "- AM: 03400146\n",
    "- Ιδιότητα: Μεταπτυχιακός φοιτητής Επιστήμης Δεδομένων και Μηχανικής Μάθησης (ΕΔΕΜΜ)\n",
    "- Ηλεκτρονική Διεύθυνση: christosnikou@mail.ntua.gr / chrisnick92@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLZdEbAy2jfr"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VsUV229__zO"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM8ivgOJAg_H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_puV3ugeAnbU"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep-MvIUCAxT8"
   },
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBKBXJDUBRUh"
   },
   "source": [
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1653109719597,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "p6lqbG9zBgdi",
    "outputId": "31b78831-31b3-44f0-a6f8-db6887a598af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX2res4JBlYb"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1653110303057,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "Gq7q944YBx0Q",
    "outputId": "34b3e8cb-869d-4464-9d0d-b72b4395cb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFF\u001b[41mH\u001b[0mFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "next_action = env.action_space.sample()\n",
    "env.step(next_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV4A7lsLB54y"
   },
   "source": [
    ">*Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gPLcFSbDnxo"
   },
   "source": [
    "Στο παρακάτω κομμάτι κώδικα επαναφέρουμε τον Agent στην αρχική του θέση και εκτελούμε τις προηγούμενες εντολές όσες φορές δείχνει η μεταβλητή `num_iters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1653110615838,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "TiA1adkWEDyG",
    "outputId": "cb5e84ed-bfbc-4867-80d4-ac8c6f8bf90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----> Move: 1 <-----\n",
      "\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 2 <-----\n",
      "\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 3 <-----\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 4 <-----\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 5 <-----\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 6 <-----\n",
      "\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 7 <-----\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mF\u001b[0mFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 8 <-----\n",
      "\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 9 <-----\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 10 <-----\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "num_iters = 10\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for i in range(num_iters):\n",
    "  print(f\"\\n{5*'-'}> Move: {i+1} <{5*'-'}\\n\")\n",
    "  next_action = env.action_space.sample()\n",
    "  env.step(next_action)\n",
    "  env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkDRm2J7EltO"
   },
   "source": [
    "Με βάση τις παραπάνω εκτελέσεις παρατηρούμε ότι ο Agent δεν επιλέγει πάντοντε να εκτελέσει την κίνηση που του ζητείται. Για παράδειγμα στην 1η κίνηση του ζητήθηκε να κινηθεί προς το κάτω μέρος του πλέγματος και αυτός παρέμεινε στην ίδια θέση. Το ίδιο παρατηρούμε και στην 3η κίνηση για παράδειγμα που του ζητήθηκε να κινηθεί προς το δεξί μέρος του πλέγματος και αυτός κινήθηκε προς τα κάτω.\n",
    "\n",
    "Όσον αφορά τη στοχαστικότητα ως προς τις κινήσεις του Agent παρατηρούμε ότι στις κινήσεις 5 και 6 ο Agent βρίσκεται στην ίδια θέση ενώ κατά την 7η κίνηση έχει μετακινηθεί μια θέση δεξιά. Αυτό δείχνει ότι οι κινήσεις του Agent <b>*δεν είναι ντετερμινιστικές*</b> καθώς στη μια περίπτωση παρέμεινε στην ίδια θέση ενώ στην 7η κίνηση μετακινήθηκε προς τα δεξιά. Συνεπώς, παρατηρούμε ότι <b>*υπάρχει στοχαστικότητα*</b> στις κινήσεις του Agent. Η στοχαστικότητα του προβλήματος φαίνεται επίσης αν εκτελέσουμε και πάλι το παραπάνω τμήμα κώδικα και παρατηρήσουμε ότι κατά τη 10η κίνηση ο Agent καταλήγει σε άλλο σημείο του πλέγματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1653111694385,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "pXdOswnqIn0x",
    "outputId": "24d6627b-cd13-47a2-dfdb-3383e5650511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----> Move: 1 <-----\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 2 <-----\n",
      "\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 3 <-----\n",
      "\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 4 <-----\n",
      "\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 5 <-----\n",
      "\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 6 <-----\n",
      "\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 7 <-----\n",
      "\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 8 <-----\n",
      "\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 9 <-----\n",
      "\n",
      "  (Up)\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "-----> Move: 10 <-----\n",
      "\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(num_iters):\n",
    "  print(f\"\\n{5*'-'}> Move: {i+1} <{5*'-'}\\n\")\n",
    "  next_action = env.action_space.sample()\n",
    "  env.step(next_action)\n",
    "  env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAL4we3gDV_J"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQKm4VAUChi1"
   },
   "source": [
    "<ul>\n",
    "<li>Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYZwC2YXGfVJ"
   },
   "source": [
    "<h2><b>Απαντήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wyb0Rb8GlCY"
   },
   "source": [
    "### Ερώτηση 1\n",
    "\n",
    ">*Μελετώντας <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDlmsDw1GwJ4"
   },
   "source": [
    "Στο πρόβλημα της παγωμένης λίμνης ο Agent προσπαθεί ξεκινώντας από το σημείο του πλέγματος με την ένδειξη Start(S) να φτάσει στο σημείο Goal(G) διασχίζοντας μόνο τα σημεία στα οποία η λίμνη είναι παγωμένη Frozen(F) χωρίς να πέσει στις τρύπες Holes(H) που υπάρχουν στη λίμνη.\n",
    "\n",
    "- Όπως φαίνεται και παρακάτω ο Agent σε κάθε σημείο του πλέγματος επιλέγει να κάνει μια από τις τέσσερις κινήσεις Up, Down, Right, Left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653111613255,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "51aAl0dVIHmo",
    "outputId": "848459b7-a862-4396-bba2-6de569b96238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of possible moves:Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"- Number of possible moves:{env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9us1SeuIzc4"
   },
   "source": [
    "Ο τρόπος με τον οποίο το πρόβλημα της παγωμένης λίμνης εντάσσεται στο πλαίσιο ενός προβλήματος βελτιστοποίσης είναι ότι ο Agent προσπαθεί να εκπαιδευτεί μέσω μιας διαδικασίας ανταμοιβής (reward) για κάθε σωστή κίνηση που εκτελεί με σκοπό να φτάσει στο σημείο της ένδειξης Goal(G). Στο συγκεκριμένο πρόβλημα η αμοιβή που δέχεται ο Agent ανάλογα με τις κινήσεις που εκτελεί φαίνεται παρακάτω.\n",
    "\n",
    "<b>*Reward schedule:*</b>\n",
    "\n",
    "- Reach goal(G): +1\n",
    "- Reach hole(H): 0\n",
    "- Reach Frozen(F): 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSf89jjWKNxQ"
   },
   "source": [
    "### Ερώτηση 2\n",
    "\n",
    ">*Να διατυπώσετε την ιδιότητα Markov. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVUMZGNFKVmh"
   },
   "source": [
    "Σε τελείως φορμαλιστικό επίπεδο η <a href=\"https://en.wikipedia.org/wiki/Markov_property\">*Μαρκοβιανή ιδιότητα*</a> μπορεί συνοψιστεί ως εξής: Αν $(\\Omega,\\, \\mathcal{F},\\, \\mathbb{P})$ είναι ένα χώρος πιθανότητας, $(S,\\mathcal{S})$ ένας μετρήσιμος χώρος και $(\\mathcal{F}_i)_{i\\in I}$ μια διήθηση$^1$ στο χώρο πιθανότητας $(\\Omega,\\, \\mathcal{F},\\, \\mathbb{P})$ τότε λέμε ότι η προσαρμοσμένη στη διήθηση  $(\\mathcal{F}_i)_{i\\in I}$ στοχαστική διαδικασία $X=\\{X_t:\\Omega\\to S\\}$ έχει τη *Μαρκοβιανή ιδιότητα* αν για κάθε $A\\in \\mathcal{S}$ και $s,t\\in I$ με $s<t$ ισχύει\n",
    "\n",
    "$$\\mathbb{P}(X_t\\in A\\,|\\,\\mathcal{F}_s)=\\mathbb{P}(X_t\\in A\\,|\\,X_s).$$\n",
    "\n",
    "Στη περίπτωση που ο χώρος $S$ είναι διακριτός και $\\mathcal{S}$ είναι η διακριτή $\\sigma$-άλγεβρα (το δυναμοσύνολο του $S$) τότε η Μαρκοβιανή ιδιότητα παίρνει την εξής γνώριμη μορφή:\n",
    "\n",
    "$$\\mathbb{P}(X_n = x_n\\,|\\, X_{n-1}=x_{n-1},\\dots,X_0=x_0)=\\mathbb{P}(X_n=x_n\\,|\\,X_{n-1}=x_{n-1}).$$\n",
    "\n",
    "\n",
    "$^1$Με τον όρο διήθηση εννοούμε ότι το σύνολο $I$ είναι ένα <a href=\"https://en.wikipedia.org/wiki/Total_order\">*πλήρως διατεταγμένο*</a> σύνολο δείκτων και κάθε $\\mathcal{F}_i$ είναι μια $\\sigma-$άλγεβρα στον $(\\Omega,\\mathbb{P})$ ώστε αν $s,t\\in I$ με $s<t$ τότε και $\\mathcal{F}_s\\subseteq \\mathcal{F}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sO4wQ93PbuS"
   },
   "source": [
    "Χονδρικά, η ιδιότητα Markov μας λέει ότι σε ένα στοχαστικό σύστημα του οποίου οι καταστάσεις περιγράφονται από το σύνολο $S$ η κατάσταση στην οποία θα βρεθεί το σύστημα στο επόμενο βήμα <b>*εξαρτάται μόνο*</b> από το σημείο στο οποίο βρίσκεται το σύστημα και <b>*όχι από τις προηγούμενες καταστάσεις*</b>. Στο πρόβλημα της παγωμένης λίμνης οι καταστάσεις $S$ είναι οι 64 θέσεις του πλέγματος πάνω στις οποίες μπορεί να κινηθεί ο Agent. Το ότι η επόμενη κίνηση που θα κάνει ο Agent εξαρτάται μόνο απ' τη θέση στην οποία βρίσκεται απλοποιεί το πρόβλημα σημαντικά καθώς για να προβλέψουμε το πως θα κινηθεί ο Agent δε χρειάζεται να λάβουμε υπ' όψιν όλο το ιστορικό των κινήσεων που έχει κάνει, πράγμα το οποίο θα ήταν αρκετά πολύπλοκλο ακόμα και στη περίπτωση του $8\\times 8$ πλέγματος."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbl5CiiJTqVC"
   },
   "source": [
    "### Ερώτηση 3\n",
    "\n",
    ">*Να περιγράψετε σύντομα τους αλγορίθμους Policy Iteration και Value Iteration, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsUcJ8RKTy06"
   },
   "source": [
    "Πριν περιγράψουμε τους δύο αλγορίθμους *Policy Iteration* και *Value Iteration* που χρησιμοποιούνται στο παρόν Notebook να πούμε δυο λόγια για το ποιο πρόβλημα προσπαθούν να λύσουν."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyeCnEndXJRK"
   },
   "source": [
    "Οι αλγόριθμοι <b>*Policy Iteration*</b> και <b>*Value Iteration*</b> προσπαθούν να βρουν τη βέλτιστη πολιτική σε ένα πρόβλημα διαδικασιών Μαρκοβιανών αποφάσεων. Πιο συγκεκριμένα, μια Μαρκοβιανή διαδικασία αποφάσεων περιγράφεται από την πεντάδα $(S,A,\\{P_{sa}\\},\\gamma,R)$ όπου:\n",
    "\n",
    "- $S$ είναι το σύνολο των καταστάσεων (States).\n",
    "\n",
    "- $A$ είναι το σύνολο των δράσεων (Actions).\n",
    "\n",
    "- $P_{sa}$ είναι οι πιθανότητες μετάβασης από μια κατάσταση σε κάποια άλλη. Πιο αναλυτικά, για κάθε $s\\in S$ και για κάθε $\\alpha \\in A, P_{s\\alpha}$ είναι μια κατανομή πάνω απ' το χώρο καταστάσεων $S$.\n",
    "\n",
    "- $\\gamma \\in [0,1)$ είναι ο παράγοντας έκπτωσης (discount factor)\n",
    "\n",
    "- $R:S\\times A\\mapsto \\mathbb{R}$ είναι η συνάρτηση αμοιβής.\n",
    "\n",
    "Η δυναμική μιας Μαρκοβιανής διαδικασίας απόφασης είναι η εξής: Αρχικά, το σύστημα ξεκινά από μια αρχική κατάσταση $s_0\\in S$ και επιλέγεται μια ενέργεια $\\alpha_0\\in A$. Ως αποτέλεσμα αυτής της ενέργειας η Μαρκοβιανή διαδικασία επιλέγει κατά τυχαίο τρόμο με βάση το μέτρο πιθανότητας $P_{s_0\\alpha_0}$ την επόμενη κατάσταση $s_1$. Σχηματικά, τη διαδικασία αυτή μπορούμε να τη φανταστούμε ως εξής:<br><br>\n",
    "\n",
    "\n",
    "$$s_0 \\overset{\\alpha_0}{\\mapsto}s_1\\overset{\\alpha_1}{\\mapsto}s_2\\overset{\\alpha_2}{\\mapsto}s_3\\overset{\\alpha_3}{\\mapsto}\\dots\\,.$$\n",
    "\n",
    "\n",
    "<br>Έχοντας επισκεφθεί τις καταστάσεις $s_0,s_1,\\dots$ με αντίστοιχες ενέργειες $\\alpha_0,\\alpha_1,\\dots,$ το συνολικό κέρδος περιγράφεται μέσω της<br><br>\n",
    "\n",
    "\n",
    "$$R(s_0,\\alpha_0)+\\gamma R(s_1,\\alpha_1) + \\gamma^2R(s_2,\\alpha_2)+\\dots\\, .$$\n",
    "\n",
    "<br>Ο στόχος της <b>*Eνισχυτικής Μάθησης (Reinforcement Learning)*</b> είναι να μεγιστοποιήσουμε το αναμενόμενο συνολικό κέρδος:<br><br>\n",
    "\n",
    "$$\\tag{3.1}\\mathbb{E}\\biggl(R(s_0,\\alpha_0)+\\gamma R(s_1,\\alpha_1)+\\gamma^2R(s_2,\\alpha_2)+\\dots \\biggr).$$<br><br>\n",
    "\n",
    "Μια <b>*πολιτική (policy)*</b> είναι μια συνάρτηση $\\pi :S\\to A$. Λέμε ότι εκτελούμε την πολιτική $\\pi$, αν οποτεδήποτε βρισκόμαστε στη κατάσταση $s\\in S$, ενεργούμε με βάση την $\\alpha = \\pi(s)$. Η <b>*συνάρτηση αποτίμησης (value function)*</b> της πολιτικής $\\pi$ δίνεται μέσω της<br><br>\n",
    "\n",
    "$$\\tag{3.2}V^{\\pi}(s) = \\mathbb{E}\\biggl(R(s_0,\\alpha_0)+\\gamma R(s_1,\\alpha_1)+\\gamma^2 R(s_2,\\alpha_2)+\\dots\\,|\\, s_0 = s,\\pi\\biggr),$$\n",
    "\n",
    "<br><br> δηλαδή, η $V^{\\pi}(s)$ είναι το αναμενόμενο κέρδος ξεκινώντας από την κατάσταση $s$ και λαμβάνοντας αποφάσεις με βάση την πολιτική $\\pi$. Δεδομένου μιας πολιτικής $\\pi$ η συνάρτηση αμοιβής $V^{\\pi}$ ικανοποιεί τις <b>*εξισώσεις Bellman*</b>:<br><br>\n",
    "\n",
    "$$\\tag{3.3}V^{\\pi}(s) = R(s) + \\gamma \\sum_{s'\\in S}P_{s\\pi(s)}V^{\\pi}(s').$$\n",
    "\n",
    "<br><br> Η <b>*βέλτιση συνάρτηση αποτίμησης (optimal value function)*</b> ορίζεται μέσω της <br><br>\n",
    "\n",
    "$$\\tag{3.4}V^*(s) = \\max_{\\pi}V^{\\pi}(s).$$\n",
    "\n",
    "<br><br> Η βέλτιση συνάρτηση αποτίμησης ικανοποιεί τις εξής εξισώσεις:<br><br>\n",
    "\n",
    "$$\\tag{3.5}V^*(s) = R(s)+\\max_{\\alpha \\in A}\\gamma \\sum_{s'\\in S}P_{sa}(s')V^*(s').$$\n",
    "\n",
    "<br><br> Τέλος, ορίζουμε τη <b>*βέλτιστη πολιτική (optimal policy)*</b> $\\pi^*:S\\to A$ ως εξής<br><br>\n",
    "\n",
    "$$\\tag{3.6} \\pi^*(s) = \\text{arg max}_{\\alpha\\in A}\\sum_{s'\\in S}P_{s\\alpha}(s')V^*(s').$$\n",
    "\n",
    "<br><br> Δηλαδή, η $\\pi^*(s)$ μας δίνει σε καθε κατάσταση $s$ την ενέργεια $\\alpha \\in A$ η οποία μεγιστοποιεί το κέρδος στον δεύτερο προσθετέο της $(3.5)$. Ο σκοπός των αλγορίθμων <b>*Policy iteration*</b> και <b>*Value Iteration*</b> είναι δεδομένου ενός πεπερασμένου συνολου $S$ και πεπερασμένων το πλήθος ενεργειών $\\alpha \\in A$ να προσδιορίσουν τη βέλτιστη πολιτική $\\pi^*$ λύνοντας τις εξισώσεις Bellman της $(3.3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuaKzUodjTvb"
   },
   "source": [
    "#### *Ο Αλγόριθμος Policy Iteration*\n",
    "\n",
    "Ο Αλγόριθμος Policy Iteration προσπαθεί να προσδιορίσει τη βέλτιστη πολιτική $\\pi^*$ μέσω της εξής επαναληπτικής διαδικασίας που περιγράφεται στον παρακάτω ψευδοκώδικα:\n",
    "\n",
    "1. Αρχικοποίηση μιας τυχαίας πολιτικής $\\pi$\n",
    "2. Επανάληψη μέχρις ότου ο αλγόριθμος να συγκλίνει:<br>\n",
    "    (α) $V\\leftarrow V^{\\pi}.$<br>\n",
    "    (β) Για κάθε κατάσταση $s$, έστω \n",
    "    \n",
    "$$\\pi(s) = \\text{arg max}_{\\alpha\\in A}\\sum_{s'\\in S}P_{s\\alpha}(s')V(s').$$\n",
    "\n",
    "3. Τέλος επανάληψης\n",
    "<br>\n",
    "\n",
    "Έτσι ο αλγόριθμος Policy Iteration υπολογίζει επαναλητικά την συνάρτηση αποτίμησης (value function) και εν συνεχεία ανανεώνει την πολιτική χρησιμοποιώντας αυτή τη συνάρτηση. Στη περίπτωση που $|S|<\\infty$ και $|A|<\\infty$ η συνάρτηση $V$ θα συγκλίνει σε πεπερασμένα βήματα στη συνάρτηση $V^*$ καθώς και η πολιτική $\\pi$ στην $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fJWw4HqPqtQ"
   },
   "source": [
    "Όπως βλέπουμε απ' τα παραπάνω ο αλγόριθμος *Policy Iteration* ξεκινώντας με μια τυχαία πολιτική ανανεώνει σε κάθε επανάληψη την πολιτική παίρνωντας σε κάθε κατάσταση την ενέργεια που μεγιστοποιεί το κέρδος μέσω της συνάρτησης αποτίμησης $V^{\\pi}$ που αντιστοιχεί στην πολιτική $\\pi$. Παρακάτω συνοψίζουμε και τον αλγόριθμο *Value Iteration*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyZmbpuGS5GQ"
   },
   "source": [
    "#### O Αλγόριθμος Value Iteration\n",
    "\n",
    "Ο Αλγόριθμος Value Iteration συνοψίζεται στον παρακάτω ψευδοκώδικα.\n",
    "\n",
    "1. Για κάθε κατάσταση $s\\in S$, θέτουμε $V(s)=0$.\n",
    "\n",
    "2. Επανάληψη μέχρις ότου ο αλγόριθμος να συγκλίνει:\n",
    "\n",
    "  Για κάθε κατάσταση $s\\in S$, ανανέωση $V(s)=R(S)+\\max_{\\alpha\\in A}\\gamma \\sum_{s'}P_{s\\alpha}(s')V(s').$\n",
    "\n",
    "3. Τέλος επανάληψης\n",
    "\n",
    "Όπως παρατηρούμε ο αλγόριθμος ανανεώνει σε κάθε επανάληψη την εκτιμώμενη συνάρτηση αποτίμησης χρησιμοποιώντας τις εξισώσεις Bellman της (3.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpZeKL9-Tgof"
   },
   "source": [
    "#### Σύγκριση των δύο αλγορίθμων\n",
    "\n",
    "Συγκρίνοντας τους δύο αλγορίθμους συνοψίζουμε τα εξής:\n",
    "\n",
    "1. Στον Policy Iteration ο αλγόριθμος περιλαμβάνει την αξιολόγηση της πολιτικής μέσω της συνάρτησης αποτίμησης και βελτιώνει σε κάθε επανάληψη την πολιτική που ακολουθείεται.\n",
    "\n",
    "2. Σε αντίθεση, ο Value Iteration βρίσκει τη βέλτιστη συνάρτηση αποτίμησης χρησιμοποιώντας επαναληπτικά τις εξισώσεις Bellman (3.5).\n",
    "\n",
    "3. Και οι δύο αλγόριθμοι συγκλίνουν στη βέλτιστη πολιτική $\\pi^*$ σε πεπερασμένα βήματα.\n",
    "\n",
    "4. Όσον αφορά την πολυπλοκότητα των δύο αλγορίθμων και οι δύο αλγόριθμοι έχουν ίδια πολυπλοκότητα. Παρ'όλα αυτά, στην πράξη για σχετικά μικρές Μαρκοβιανές διαδικασίες απόφασης ο αλγόριθμος Policy Iteration είναι αρκετά γρήγορος και συγκλίνει στη βέλτιστη πολιτική $\\pi^*$ σε λίγες επαναλήψεις. Όμως, για Μαρκοβιανές διαδικασίες απόφασης με μεγάλο χώρο καταστάσεων ο ακριβής υπολογισμός της αποτίμησης $V^{\\pi}$ είναι αρκετά χρονοβόρος και έτσι σε αυτή την περίπτωση ο Value Iteration μπορεί να είναι καλύτερη επιλογή."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Muqg8nLKV5PW"
   },
   "source": [
    "### Ερώτηση 4\n",
    "\n",
    ">*Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους Policy Iteration και Value Iteration αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή time. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpeINLinWAif"
   },
   "source": [
    "Παρακάτω εκτελούμε τους δύο αλγορίθμους για το πρόβλημα της παγωμένης λίμνης και συγκρίνουμε τους δύο αλγορίθμους ως προς το χρόνο εκτέλεσης και τα αποτελέσματά τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZQzzXjfWM_W"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6mci5P4HJ_1"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4108,
     "status": "ok",
     "timestamp": 1653132108411,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "_43MjfJ9G8v7",
    "outputId": "55c34dd9-2693-4799-b97c-714ad4390617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 12.\n",
      "Average scores =  1.0\n",
      "- Execution time for Policy Iteration: 3.386103 sec(s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "tic = time.time()\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(scores))\n",
    "tac = time.time()\n",
    "print(f\"- Execution time for Policy Iteration: {(tac-tic):.6f} sec(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcikBq6BHRQM"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2579,
     "status": "ok",
     "timestamp": 1653132146469,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "gHvcnTDcHGmH",
    "outputId": "53099ae0-121b-4ad7-ab65-3f9deff146e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0\n",
      "- Execution time for Value Iteration: 2.429732 sec(s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "tic = time.time()\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print('Policy average score = ', policy_score)\n",
    "tac = time.time()\n",
    "print(f\"- Execution time for Value Iteration: {(tac-tic):.6f} sec(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjYyNL6YWw3O"
   },
   "source": [
    "Στον παρακάτω πίνακα συνοψίζουμε τα αποτελέσματα των δύο αλγορίθμων.\n",
    "\n",
    "| **Αλγόριθμος** | **Αριθμός Επαναλήψεων** |**Χρόνος Εκτέλεσης (sec)**|\n",
    "|:---------------|:-----------------|:-----------------|\n",
    "| **Policy Iteration** |       **12**     |3.386103|\n",
    "| **Value Iteration**      |       **2357**      |2.429732|\n",
    "\n",
    "Όπως βλέπουμε και απ' τον παραπάνω πίνακα ο αλγόριθμος Policy Iteration συγκλίνει γρηγορότερα από τον Value Iteration στη βέλτιστη πολιτική. Παρ' όλα αυτα ο αλγόριθμος Value Iteration είναι γρηγορότερος κατά 1 δευτερόλεπτο από τον Policy Iteration.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_7_Markov_Decision_Processes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
