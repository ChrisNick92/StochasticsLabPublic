{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPt3OeKRpH1f"
   },
   "source": [
    "# Στοχαστικές Διεργασίες και Βελτιστοποίηση στη Μηχανική Μάθηση\n",
    "\n",
    "## 7o Εργαστήριο - *Q-Learning*\n",
    "\n",
    "- Ονομ/νυμο: Χρήστος Νίκου\n",
    "- AM: 03400146\n",
    "- Ιδιότητα: Μεταπτυχιακός φοιτητής Επιστήμης Δεδομένων και Μηχανικής Μάθησης (ΕΔΕΜΜ)\n",
    "- Ηλεκτρονική Διεύθυνση: christosnikou@mail.ntua.gr / chrisnick92@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHFOjD_VY0ld"
   },
   "source": [
    "<h1>Q-Learning</h1>\n",
    "\n",
    "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείγματος θα εξετάσετε μία υλοποίηση με Q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
    "<ul>\n",
    "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
    "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
    "<li>Να τηρούνται οι κανόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
    "</ul>\n",
    "\n",
    "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n",
    "\n",
    "<h4>Επιβράβευση</h4>\n",
    "<ul>\n",
    "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
    "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
    "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
    "</ul>\n",
    "\n",
    "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
    "\n",
    "<h4>Πλήθος Καταστάσεων</h4>\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
    "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
    "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
    "\n",
    "<h4>Πλήθος Ενεργειών</h4>\n",
    "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
    "<ul>\n",
    "<li>0=Νότια</li>\n",
    "<li>1=Βόρεια</li>\n",
    "<li>2=Ανατολικά</li>\n",
    "<li>3=Δυτικά</li>\n",
    "<li>4=Επιβίβαση</li>\n",
    "<li>5=Αποβίβαση</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOQpQ0DygDzt"
   },
   "source": [
    "><b>1.</b> *Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QScLEJFRpzQ6"
   },
   "source": [
    "Ο αλγόριθμος *Q-Learning* είναι ένας *model-free* αλγόριθμος ο οποίος προσπαθεί να βρει τη βέλτιστη πολιτική σε ένα πρόβλημα ενισχυτικής μάθησης <b>*χωρίς να έχει επίγνωση των πιθανοτήτων μετάβασης και των συναρτήσεων αποτίμησης του περιβάλλοντος στο οποίο εφαρμόζεται*</b>. Αυτή είναι και η <b>*βασική διαφορά*</b> του αλγορίθμου Q-Learning από τους αλγορίθμους Policy και Value Iteration. Δηλαδή, το ότι οι δύο τελευταίοι προϋποθέτουν τη γνώση των πιθανοτήτων μετάβασης καθώς και της συνάρτησης αποτίμησης. Οι αλγόριθμοι Policy και Value Iteration μελετήθηκαν στο πρώτο μέρος του 7ου εργαστηρίου. Σε αυτό το σημείο θα δώσουμε μια συντομή περιγραφή του αλγορίθμου *Q-Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOHvfsvK6siF"
   },
   "source": [
    "#### Ο Αλγόριθμος *Q-Learning*\n",
    "\n",
    "Σκοπός του αλγορίθμου *Q-Learning* είναι να προσδιορίσει τη συνάρτηση $Q^*(s,\\alpha)$ η οποία δίνει τη συνολική αναμενόμενη αποτίμηση λαμβάνοντας υπ'όψιν τον παράγοντας έκπτωσης $\\gamma$ (cumulative discounted reward) στη περίπτωση που ο Agent στην κατάσταση $s\\in S$ ενεργεί κατά την $\\alpha\\in A^1$. Κατά την εκκίνηση του αλγορίθμου ορίζεται ένας πίνακας $Q(s,\\alpha)$ με $s\\in S,\\,\\alpha \\in A$ ο οποίος αρχικοποιείται με μηδενικές τιμές. Ο πίνακας $Q(s,\\alpha)$ εκφράζει την εκτίμηση του αλγορίθμου για την βέλτιστη τιμή $Q^*(s,\\alpha)$. Η συνάρτηση $Q(s_t,\\alpha_t)$ για μια δεδομένη χρονική στιγμή $t$ δίνεται μέσω της<br><br>\n",
    "\n",
    "$$Q^{\\pi}(s_t,\\alpha_t)=\\mathbb{E}\\biggl(R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+\\dots\\,|\\,s_t,\\alpha_t\\biggr).$$\n",
    "\n",
    "<br><br> Η διαδικασία εκμάθησης και ανανέωσης της συνάρτησης $Q(s,\\alpha)$ με σκοπό την προσέγγιση της $Q^*(s,\\alpha)$ είναι η εξής:\n",
    "\n",
    "1. Αρχικά ορίζεται ο πίνακας $Q(s,\\alpha)$ ο οποίος είναι διαστάσεων $|S|\\times |A|$ και αρχικοποιείται με μηδενικές τιμές.\n",
    "\n",
    "2. Ξεκινάει ο Agent από μια αρχική κατάσταση $s_0\\in S$ και ενεργεί με βάση το σύνολο $A$. Δηλαδή, επιλέγεται ένα $\\alpha\\in A$ και πραγματοποιείται μια κίνηση.\n",
    "\n",
    "3. Η ανανέωση της συνάρτησης $Q(s,\\alpha)$ γίνεται μέσω της σχέσης<br><br>\n",
    "\n",
    "$$\\tag{1}Q_{new}(s,\\alpha) = \\underbrace{Q_{current}(s,\\alpha)}_{\\text{current Q values}}+\\underbrace{\\alpha}_{\\text{learning rate}}\\cdot \\biggl(\\underbrace{R(s,\\alpha)}_{\\text{Reward}}+\\underbrace{\\gamma}_{Discount Rate}\\cdot\\max_{\\alpha \\in A} Q(s',\\alpha')-Q_{\\text{current}}(s,\\alpha)\\biggr)$$<br><br>\n",
    "\n",
    "Όπως βλέπουμε και απο τη σχέση $(1)$ δεν υπεισέρχεται κάπου η πληροφορία για τις πιθανότητες μετάβασης και τη συνάρτηση αποτίμησης $V$. Το μόνο που λαμβάνει ως πληροφορία ο αλγόριθμος είναι από το περιβάλλον είναι η συνάρτηση αποτίμησης $R:S\\times A\\to \\mathbb{R}$.<br><br>\n",
    "\n",
    "$^1$ Με $S$ συμβολίζουμε το σύνολο των καταστάσεων και με $A$ το σύνολο των ενεργειών.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z5hOOH9Lpv2"
   },
   "source": [
    "#### Λίγα λόγια για την ενισχυτική μάθηση\n",
    "\n",
    "Στην ενισχυτική μάθηση εντάσσονται τα προβλήματα που λύνονται λαμβάνοντας αποφάσεις και εξετάζοντας την επίδραση που έχουν οι αποφάσεις αυτές στο περιβάλλον που ορίζεται το εκάστοτε πρόβλημα. Το περιβάλλον περιγράφεται από ένα σύνολο καταστάσεων $S$ και ένα σύνολο ενεργειών $A$. Επίσης, δίνεται και μια συνάρτηση $R:S\\times A\\to \\mathbb{R}$ η οποία αντιστοιχεί στην αμοιβή που επιφέρει μια ενέργεια $\\alpha \\in A$ όταν βρισκόμαστε στην κατάσταση $s\\in S$. Ο σκοπός της ενισχυτικής μάθησης είναι να εκπαιδεύση έναν πράκτορα (agent) έτσι ώστε αυτός ξεκινώντας από μια κατάσταση $s\\in S$ να λαμβάνει τις ενέργειες $\\alpha \\in A$ οι οποίες να μεγιστοποιούν τη συνάρτηση αποτίμησης (λαμβάνοντας υπ'όψιν έναν παράγοντα έκπτωσης $\\gamma$).<br><br>\n",
    "\n",
    "Τα προβλήματα στα οποία βρίσκει εφαρμογή η ενισχυτική μάθηση είναι η αυτόματη οδήγηση, αυτόματη μετάφραση και στα παιχνίδια εικονική πραγματικότητας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc11bCDZgfSx"
   },
   "source": [
    "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3040,
     "status": "ok",
     "timestamp": 1653146617177,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "y_95lIAzWYpp",
    "outputId": "4582e132-1b20-4bb0-ec38-febb7b5701de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
      "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
      "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1653146618233,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "NDHLb5PGWdwr",
    "outputId": "9d227632-c6a8-4308-fc4a-66669d9ba09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1653146620603,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "396kTtOrW-cO",
    "outputId": "ac31d377-4ddc-4db5-f450-09f1bc1640a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UC6-XuIhF5_"
   },
   "source": [
    "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1653146623218,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "nPSOw5CdXFx1",
    "outputId": "7599da89-ef78-4395-81d1-b04e0f66d7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsAkQJhchVFy"
   },
   "source": [
    "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1653146625960,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "j7oDbznIXOJo",
    "outputId": "dfbc6616-f369-4570-fc12-845235b41058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtROeD1ph6kk"
   },
   "source": [
    "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
    "\n",
    "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1653146634149,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "vKHhcDxVXUFz",
    "outputId": "26edecda-b2c8-4a32-ec0a-fdda67f6ba06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 936\n",
      "Penalties incurred: 277\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aAS9pBJN9UP"
   },
   "source": [
    "><b>*2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;*</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cE797FWOe22"
   },
   "source": [
    "Όπως βλέπουμε σε μια εκτέλεση του αλγορίθμου που επιλέγει κατά τυχαίο τρόπο μια κίνηση χρειάστηκαν 936 βήματα μέχρις ότου ο πράκτορας να φτάσει στην επιθυμητή κατάσταση και επιβλήθηκαν 277 ποινές. Η χρήση του αλγορίθμου Q-Learning στο συγκεκριμένο πρόβλημα θα μας βοηθούσε έτσι ώστε ο πράκτορας να εκπαιδεύεται κατά τη διάρκεια του αλγορίθμου με αποτέλεσμα να κάνει λιγότερα λάθη."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj3s09rsizVm"
   },
   "source": [
    "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
    "\n",
    "\n",
    ">*3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;*\n",
    "\n",
    "Υπενθυμίζουμε τη σχέση (1) που περιγράφει την ανανέωση του πίνακα $Q(s,\\alpha)$:\n",
    "<br><br>\n",
    "\n",
    "$$\\tag{2}Q_{\\text{new}}(s,\\alpha)=Q_{\\text{current}}(s,\\alpha)+\\alpha\\cdot \\biggl(R(s,\\alpha)+\\gamma\\cdot \\max_{\\alpha \\in A} Q(s',t)-Q(s,\\alpha)\\biggr)$$.<br><br>\n",
    "\n",
    "- H παράμετρος $\\alpha$ αντιστοιχεί στο βαθμό εκμάθησης και είναι μια τιμή μεταξύ του $(0,1]$. Τιμές του $\\alpha$ που βρίσκονται στο $0$ τείνουν να μηδενίσουν τον προσθετέτο στο δεξί μέλος της $(2)$ πράγμα που σημαίνει ότι οι τιμές του πίνακα $Q$ δεν επηρεάζονται σημαντικά από τη νέα πληροφορία που επιφέρουν οι μελλοντικές καταστάσεις. Αντιθέτως, τιμές κοντά στο $1$ επηρέαζουν σημαντικά την ανανέωση του $Q$ ανάλογα με τη νέα πληροφορία που υπεισέρχεται εξαιτίας των επόμενων κινήσεων.\n",
    "\n",
    "- Η παράμετρος $\\gamma$ είναι ο ρυθμός έκπτωσης ο οποίος επηρεάζει την αμοιβή των μελλοντικών ενεργειών του πράκτορα. Ο λόγος ύπαρξης αυτής της παραμέτρου είναι έτσι ώστε ο πράκτορας να επιλέγει τις ενέργεις που επιφέρουν υψηλές επιβραβεύσεις σε λίγα βήματα (ιδανικά στο αμέσως επόμενο βήμα). Τιμές της παραμέτρου κοντά στο $1$ τείνουν τον πράκτορα να επιλέγει όσον το δυνατό συντομότερα τις ενέργειες που του επιφέρουν τις μεγαλύτερες επιβραβεύσεις.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1653148052726,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "JJk3NTfcXrrA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57766,
     "status": "ok",
     "timestamp": 1653148110884,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "Oy2Yg8DTXtHW",
    "outputId": "063fd849-1780-4a11-ce24-cf828ab71989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 55.8 s, sys: 11.1 s, total: 1min 6s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1653140552357,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "dxt4fmvGYBOm",
    "outputId": "ed8b756c-8a68-45d3-92d9-dc37719f8da6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40614917,  -2.27325184,  -2.40884107,  -2.36058625,\n",
       "       -10.432459  , -10.68623986])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTGrTEyGTbig"
   },
   "source": [
    "Έχοντας εκπαιδεύσει τον πράκτορα εξετάζουμε το πόσο γρήγορα ο πράκτορας φτάνει στην επιθυμητή κατάσταση. Εκτελούμε το πείραμα για 100 επεισόδια και συγκρίνουμε τα αποτελέσματα με τον αλγόριθμο που επιλέγει κινήσεις τυχαία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1653148847433,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "6W9JE9yOYGgP",
    "outputId": "1220cbbb-0b3e-4d74-f242-675f4c172ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.82\n",
      "Average penalties per episode: 0.0\n",
      "Average reward per move: 0.6380655226209049\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        total_rewards+=reward\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(f\"Average reward per move: {total_rewards/total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14604,
     "status": "ok",
     "timestamp": 1653148881518,
     "user": {
      "displayName": "Christos Nikou",
      "userId": "10017729997489791278"
     },
     "user_tz": -180
    },
    "id": "4RzTvyA1T4qv",
    "outputId": "2f0f5336-a80e-4532-c414-6719da008955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 1818.76\n",
      "Average penalties per episode: 590.64\n",
      "Average reward per move: -3.911192240867404\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance with random moves\"\"\"\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0,0,0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "  env.s = 328  # set environment to illustration's state\n",
    "\n",
    "  epochs = 0\n",
    "  penalties, reward = 0, 0\n",
    "\n",
    "  frames = [] # for animation\n",
    "\n",
    "  done = False\n",
    "\n",
    "  while not done:\n",
    "      action = env.action_space.sample()\n",
    "      state, reward, done, info = env.step(action)\n",
    "\n",
    "      if reward == -10:\n",
    "          penalties += 1\n",
    "      \n",
    "      # Put each rendered frame into dict for animation\n",
    "      frames.append({\n",
    "          'frame': env.render(mode='ansi'),\n",
    "          'state': state,\n",
    "          'action': action,\n",
    "          'reward': reward\n",
    "          }\n",
    "      )\n",
    "      total_rewards+=reward\n",
    "      epochs+=1\n",
    "  total_penalties += penalties\n",
    "  total_epochs += epochs\n",
    "    \n",
    "    \n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(f\"Average reward per move: {total_rewards/total_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R7gW1nLj-qE"
   },
   "source": [
    "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
    "<ul>\n",
    "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
    "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
    "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
    "</ul>\n",
    "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbUWti6oU8xv"
   },
   "source": [
    "Στον παρακάτω πίνακα βλέπουμε τα αποτελέσματα των δύο αλγορίθμων, Q-Learning και του αλγορίθμου που επιλέγει με τυχαίο τρόπο όσον αφορά:\n",
    "\n",
    "1. Τον μέσο αριθμό παραβάσεων ανά επεισόδιο\n",
    "2. Τον μέσο αριθμό βημάτων ανά διαδρομή και \n",
    "3. Τον μέσο αριθμό ανταμοιβών ανά κίνηση.\n",
    "\n",
    "| **Αλγόριθμος** | **Μέσος αριθμός παραβάσεων** |**Μέσος αριθμός βημάτων**|**Μέσος αριθμός ανταμοιβών ανά κίνηση**|\n",
    "|:---------------|:-----------------|:-----------------|:-----------------|\n",
    "| **Q-Learning** |     0       | 12.82 | 0.6380 |\n",
    "| **Random**      |       590.64     | 1818.76 | -3.9111  |\n",
    "\n",
    "\n",
    "Όπως βλέπουμε και απ' τον παραπάνω πίνακα η υπεροχή του αλγορίθμου Q-Learning είναι εμφανής ως προς όλες τις μετρικές. Όπως βλέπουμε ο αλγόριθμος Q-Learning κάνει 0 παραβάσεις κατά μέσο όρο και τελειώνει σε περίπου 12 βήματα με μέσο αριθμό ανταμοιβής 0.63. Αντιθέτως, ο αλγόριθμος που επιλέγει με τυχαίο τρόπο βλέπουμε ότι κάνει 590 παραβάσεις κατά μέσο, ολοκληρώνει την αποστολή σε περίπου 1818 βήματα έχοντας -3.9 ζημιά."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_7_Q_Learning_2022).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
